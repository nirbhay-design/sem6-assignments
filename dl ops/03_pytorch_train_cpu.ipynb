{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd088965",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60af001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7fa0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700c00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Monitoring with Wandb\n",
    "import wandb\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c83999",
   "metadata": {},
   "source": [
    "### Login to [Wandb](https://wandb.ai/home) \n",
    "\n",
    "Save API Key once login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acc4bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_867/1246793673.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(anonymous, key, relogin, host, force)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mconfigured\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfigured\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, _backend, _silent, _disable_warning)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# make sure login credentials get to the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 )\n\u001b[1;32m    119\u001b[0m             )\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             )\n\u001b[0;32m--> 989\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "##################\n",
    "wandb.login()\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53374d71",
   "metadata": {},
   "source": [
    "Create a Model Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(\"./saved\")\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4db42",
   "metadata": {},
   "source": [
    "Neccesaary and Performance Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    #Neccessary\n",
    "    TRAIN_CSV = \"../data/train.csv\",\n",
    "    TEST_CSV = \"../data/test.csv\",\n",
    "    IMAGE_PATH= \"../data/images\",\n",
    "    VOCAB = \"labels.json\",\n",
    "    saved_path=\"./saved/resnet18.pt\",\n",
    "    lr=0.001, \n",
    "    EPOCHS = 2,\n",
    "    BATCH_SIZE = 2,\n",
    "    IMAGE_SIZE = 224,\n",
    "    TRAIN_VALID_SPLIT = 0.2,\n",
    "    SEED = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701af1c4",
   "metadata": {},
   "source": [
    "### Initiate a Wandb Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the Project and Entity\n",
    "wandb.init(project=\"pytorch-lab\", config=config)\n",
    "# access all HPs through wandb.config, so logging matches execution!\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21052f73",
   "metadata": {},
   "source": [
    "### Data Manipulation (Can be written Separately too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config.TRAIN_CSV)\n",
    "test_df = pd.read_csv(config.TEST_CSV)\n",
    "f = open(config.VOCAB)\n",
    "vocab = json.load(f)\n",
    "\n",
    "df_fnames = train_df[\"image_id\"].append(test_df[\"image_id\"],ignore_index=True).tolist()\n",
    "def create_fname(path,extension):\n",
    "    def add_extension(fname):\n",
    "        return os.path.join(path,fname)+extension\n",
    "    return add_extension\n",
    "\n",
    "jpeg_extension_creator = create_fname(config.IMAGE_PATH,\".jpg\")\n",
    "train_df[\"image_id\"] = train_df[\"image_id\"].apply(jpeg_extension_creator)\n",
    "test_df[\"image_id\"] = test_df[\"image_id\"].apply(jpeg_extension_creator)\n",
    "for label in vocab:\n",
    "    train_df.loc[train_df[label] == 1, \"label\" ] = vocab[label] \n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff2a9a",
   "metadata": {},
   "source": [
    "Data Split: Train and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_X, valid_df_X, train_df_y, valid_df_y = train_test_split(train_df[\"image_id\"],\n",
    "                                                                  train_df[\"label\"], \n",
    "                                                                  test_size=config.TRAIN_VALID_SPLIT, \n",
    "                                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_split = pd.DataFrame(data={\"image_id\": train_df_X, \"label\": train_df_y})\n",
    "train_df_split.to_csv(\"../data/train_split.csv\", sep=',',index=False)\n",
    "\n",
    "valid_df_split = pd.DataFrame(data={\"image_id\": valid_df_X, \"label\": valid_df_y})\n",
    "valid_df_split.to_csv(\"../data/val_split.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c43a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train input samples is {}\".format(len(train_df_X)))\n",
    "print(\"Number of valid input samples is {}\".format(len(valid_df_X)))\n",
    "print(\"Number of train output samples is {}\".format(len(train_df_y)))\n",
    "print(\"Number of valid output samples is {}\".format(len(valid_df_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(Image.open(train_df_X[0])).dtype "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe65c5",
   "metadata": {},
   "source": [
    "```\n",
    "--> Image_File_Path (String) \n",
    "--> Image.open(File_Path) \n",
    "--> np.array(Image.open(File_Path))\n",
    "--> Images [0-255] uint8 \n",
    "--> [0-1]; float32 \n",
    "--> x - Mean_training_dataset  / Std_training_dataset```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974235c",
   "metadata": {},
   "source": [
    "Apply Data Transforms (Aumentations + Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop((config.IMAGE_SIZE,config.IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((config.IMAGE_SIZE,config.IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a17ab7",
   "metadata": {},
   "source": [
    "Custom Dataset and Dataloader for Plant Pathology Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantPathologyDataset(Dataset):\n",
    "    def __init__(self,x,y,vocab,transforms):\n",
    "        self.x = x # File Path in CSV\n",
    "        self.y = y # Label in CSV\n",
    "        self.vocab = vocab # Dictionary\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self,idx): #File Name --> Preprocessed 3-D Tensor\n",
    "        fname = self.x.iloc[idx]        \n",
    "        label = self.y.iloc[idx]\n",
    "        image = Image.open(fname)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, label #[3,224,224], [0-3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187657c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PlantPathologyDataset(train_df_X, \n",
    "                                 train_df_y, \n",
    "                                 vocab,\n",
    "                                 data_transforms[\"train\"])\n",
    "valid_ds = PlantPathologyDataset(valid_df_X, \n",
    "                                 valid_df_y,\n",
    "                                 vocab,\n",
    "                                 data_transforms[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21152514",
   "metadata": {},
   "source": [
    "Optimizers:\n",
    "Gradient Descent:-\n",
    "    a. Stoicastic Gradient Descent bs = 1; 'n' number of examples. 'n / 1' number of data loader/steps for 1 Epoch\n",
    "    b. Mini-Batch Gradient Descent bs = 32; 'n' number of examples. 'n / 32' number of dataloaders/step for 1 Epoch \n",
    "    c. Full Batch Gradient Descent bs = total_number_of_samples number of dataloader/steps = 1 for 1 Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705cfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0][0].shape #3,224,224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20058fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Iterations\n",
    "1456 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "train_dl = DataLoader(train_ds,\n",
    "                      batch_size=config.BATCH_SIZE,\n",
    "                      shuffle=True,)\n",
    "valid_dl = DataLoader(valid_ds,\n",
    "                      batch_size=config.BATCH_SIZE,\n",
    "                      shuffle=False,)\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5e126",
   "metadata": {},
   "source": [
    "Load Model : Pretrained from torchvision model zoo or Saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "#Modify the classifier for agriculture data\n",
    "num_ftrs = model.fc.in_features\n",
    "print(num_ftrs)\n",
    "model.fc = nn.Sequential(nn.Linear(num_ftrs,512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.3),\n",
    "                        nn.Linear(512,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa91cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BackPropagation & Optimization\n",
    "## W_new = W_old - LR * W_gradient ; Gradient Descent Optimization Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb528f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=config.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2a59d",
   "metadata": {},
   "source": [
    "CrossEntropyLoss = Softmax(Final Activation Function for Normalizing the output of the FC Layer) + Negative Log Likelihood (NLL) Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d391e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22358608",
   "metadata": {},
   "source": [
    "### Training Pipeline Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion,optimizer,num_epochs=10):\n",
    "    ############################################################\n",
    "    # tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    ############################################################\n",
    "\n",
    "    since = time.time()                                            \n",
    "    batch_ct = 0\n",
    "    example_ct = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        #Training\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for x,y in train_dl: #BS=32 ([BS,3,224,224], [BS,4])            \n",
    "  \n",
    "            #######################################################################\n",
    "            # The second code snippet does not zero the memory of each individual parameter, \n",
    "            # also the subsequent backward pass uses assignment instead of addition to store gradients,\n",
    "            # this reduces the number of memory operations.\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            train_logits = model(x) #Input = [BS,3,224,224] (Image) -- Model --> [BS,4] (Output Scores)\n",
    "            _, train_preds = torch.max(train_logits, 1)\n",
    "            train_loss = criterion(train_logits,y)\n",
    "\n",
    "            train_loss.backward() # Backpropagation this is where your W_gradient\n",
    "            loss=train_loss\n",
    "\n",
    "            optimizer.step() # W_new = W_old - LR * W_gradient \n",
    "            example_ct += len(x) \n",
    "            batch_ct += 1\n",
    "            \n",
    "            ########################################################################\n",
    "            # Stores Wandb Logs here\n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_ct + 1) % 25) == 0:\n",
    "                train_log(loss, example_ct, epoch)\n",
    "            print(\"iteration: {}\".format(i))\n",
    "            i = i+1\n",
    "            ########################################################################\n",
    "        epoch_time2 = time.time()\n",
    "        #validation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        # Disable gradient calculation for validation or inference using torch.no_rad()\n",
    "        with torch.no_grad():\n",
    "            for x,y in valid_dl:\n",
    "                valid_logits = model(x)\n",
    "                _, valid_preds = torch.max(valid_logits, 1)\n",
    "                valid_loss = criterion(valid_logits,y)\n",
    "                running_loss += valid_loss.item() * x.size(0)\n",
    "                running_corrects += torch.sum(valid_preds == y.data)\n",
    "                total += y.size(0)\n",
    "                ########################################################################\n",
    "                # Test Accuracy Logs\n",
    "                wandb.log({\"test_accuracy\": running_corrects / total})\n",
    "                ########################################################################\n",
    "            \n",
    "        epoch_loss = running_loss / len(valid_ds)\n",
    "        epoch_acc = running_corrects.double() / len(valid_ds)\n",
    "        print(\"Validation Loss is {}\".format(epoch_loss))\n",
    "        print(\"Validation Accuracy is {}\".format(epoch_acc.cpu()))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    torch.save(model.state_dict(), config.saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d939a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, criterion, optimizer, num_epochs=config.EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d6b07",
   "metadata": {},
   "source": [
    "## Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
