# -*- coding: utf-8 -*-
"""assn2-que2b.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LDdP7YL8t5VTSDFIjB2ELNkl8eNNFKAl
"""

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import torch.optim as optim
import random
import time
import os
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score,precision_recall_fscore_support,auc
from sklearn.preprocessing import label_binarize
import warnings
warnings.filterwarnings('ignore')

config = dict(
    train_data = "../../../dataset/miniimgnet_dlassignment/tinyimgnet/tiny-imagenet-200/train",
    test_data = "../../../dataset/miniimgnet_dlassignment/tinyimgnet/tiny-imagenet-200/val/images",
    test_anno="../../../dataset/miniimgnet_dlassignment/tinyimgnet/tiny-imagenet-200/val/val_annotations.txt",
    wnids = "../../../dataset/miniimgnet_dlassignment/tinyimgnet/tiny-imagenet-200/wnids.txt",
    lr = 0.001,
    EPOCHS = 50,
    BATCH_SIZE = 32,
    IMAGE_SIZE = 64,
    pin_memory = True,
    num_workers = 2,
    SEED = 42,
    GPU_ID=0,
    val_split = 0.2
)

class CustomTestDataset():
  def __init__(self,img_path):
    self.img_path = img_path
    with open(config['wnids']) as f:
      self.wnids = f.read().split('\n')
      self.wnids.remove('')

    with open(config['test_anno']) as f:
      self.test_anno = list(map(lambda x:x.split('\t')[:2],f.read().split("\n")))
      self.test_anno.remove([''])

    self.wnids = sorted(self.wnids,key = lambda x:x)
    self.mapping = dict(list(zip(self.wnids,list(range(200)))))
    # self.rev_mapping = {j:i for i,j in self.mapping.items()}
    self.transformations = transforms.ToTensor()

  def __len__(self):
    return len(self.test_anno)

  def __getitem__(self,idx):
    test_img, class_name = self.test_anno[idx]
    cls_idx = self.mapping.get(class_name,-1)
    
    img = Image.open(os.path.join(config['test_data'],test_img)).convert('RGB')
    img = self.transformations(img)
    return (img,cls_idx)

class CustomTrainDataset():
  def __init__(self,img_path):
    self.img_path = img_path
    with open(config['wnids']) as f:
      self.wnids = f.read().split('\n')
      self.wnids.remove('')
    self.wnids = sorted(self.wnids,key = lambda x:x)
    self.mapping = dict(list(zip(self.wnids,list(range(200)))))

    img_class = os.listdir(self.img_path)
    self.img_map = []
    for clss in img_class:
      cls_imgs = os.listdir(os.path.join(self.img_path,clss,'images'))
      clss_imgs = list(map(lambda x:[clss,x],cls_imgs))
      self.img_map.extend(clss_imgs)
    
    self.transformations = transforms.ToTensor()
  def __len__(self):
    return len(self.img_map)

  def __getitem__(self,idx):
    class_image,image_name = self.img_map[idx]
    cls_idx = self.mapping.get(class_image,-1)

    img = Image.open(os.path.join(self.img_path,class_image,'images',image_name)).convert('RGB')
    img = self.transformations(img)

    return (img,cls_idx)

device = torch.device(f'cuda:{config["GPU_ID"]}' if torch.cuda.is_available() else 'cpu')
print(device)

config['DEVICE'] = device

torch.manual_seed(config['SEED'])
torch.cuda.manual_seed(config['SEED'])


transformations = {
  'train':torchvision.transforms.Compose([
          torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
  ]),
  'test':torchvision.transforms.Compose([
          torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
  ]),
}

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"  

torch.backends.cudnn.benchmarks = True
torch.backends.cudnn.deterministic = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

def get_params(model):
  par = 0
  for params in model.parameters():
    if (params.requires_grad == True):
      par += params.numel()
  return par

class D_ce(nn.Module):
  def __init__(self,num_class=200):
    super(D_ce,self).__init__()
    self.n_class = num_class
    self.dnet = torchvision.models.resnet18(pretrained=True)
    self.dnet.fc = nn.Linear(512,self.n_class)
    print(get_params(self.dnet))

    for name,params in self.dnet.named_parameters():
      if (name == "layer4.0.conv1.weight"):
        break
      params.requires_grad = False

    print(get_params(self.dnet))

  def forward(self,x):
    x = self.dnet(x)
    return x

def train(model,train_load,lossfunction,optimizer,n_epochs=200):
    tval = {'trainacc':[],"trainloss":[]}
    starttime = time.time()
    for epochs in range(n_epochs):
        model.train()
        cur_loss = 0
        curacc = 0
        len_train = len(train_load)
        for idx , (data,target) in enumerate(train_load):
            data = transformations['train'](data)    
            data = data.to(device)
            target = target.to(device)
            # model = model.to(device)

            scores = model(data)    
            loss = lossfunction(scores,target)
            cur_loss += loss.item() / (len_train)
            scores = F.softmax(scores,dim = 1)
            _,predicted = torch.max(scores,dim = 1)
            correct = (predicted == target).sum()
            samples = scores.shape[0]
            curacc += correct / (samples * len_train)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            print('TrainBatchDone:{:d}'.format(idx),end='\r') 
  
        tval['trainacc'].append(float(curacc))
        tval['trainloss'].append(float(cur_loss))
      
        print('epoch:[{:d}/{:d}], TrainAcc:{:.3f}, TrainLoss:{:.3f}'.format(epochs+1,n_epochs,curacc,cur_loss)) 

    time2 = time.time() - starttime
    print('done time {:.3f} Mins'.format(time2/60))
    return tval

def evaluate(model,loader,name='test'):
  model.eval()
  correct = 0;samples =0
  pre_prob = []
  lab = []
  predicted_labels = []

  with torch.no_grad():
      for idx,(x,y) in enumerate(loader):
          x = transformations['test'](x)
          x = x.to(device)
          y = y.to(device)
          # model = model.to(device)

          scores = model(x)
          predict_prob = F.softmax(scores)
          _,predictions = predict_prob.max(1)

          predictions = predictions.to('cpu')
          y = y.to('cpu')
          predict_prob = predict_prob.to('cpu')

          predicted_labels.extend(list(predictions.numpy()))
          pre_prob.extend(list(predict_prob.numpy()))
          lab.extend(list(y.numpy()))

          correct += (predictions == y).sum()
          samples += predictions.size(0)

          # print('batches done : ',idx,end='\r')
      
      print(f'correct are {correct/samples:.3f}')
      
  model.train()
  return np.array(lab),np.array(predicted_labels),np.array(pre_prob)

def plot_loss_acc(tval):
  plt.figure(figsize=(5,4))
  plt.plot(list(range(1,config['EPOCHS']+1)),tval['trainloss'],label='loss')
  plt.plot(list(range(1,config['EPOCHS']+1)),tval['trainacc'],label='accuracy')
  plt.xlabel('epochs')
  plt.ylabel('loss/accuracy')
  plt.title('loss_accuracy')
  plt.legend()
  plt.savefig('visual_curves/loss_acc_2b.svg',format='svg')
  plt.show()

ctd = CustomTestDataset(config['test_data'])
test_loader = torch.utils.data.DataLoader(ctd,batch_size=config['BATCH_SIZE'],shuffle=True,num_workers=config['num_workers'],pin_memory=config['pin_memory'])
cTd = CustomTrainDataset(config['train_data'])
train_set,_ = torch.utils.data.dataset.random_split(cTd,[50000,50000])
train_loader = torch.utils.data.DataLoader(cTd,batch_size=config['BATCH_SIZE'],shuffle=True,num_workers=config['num_workers'],pin_memory=config['pin_memory'])

print(len(train_loader))
print(len(test_loader))

dnet = D_ce()
dnet = dnet.to(device)

lossfunction = nn.CrossEntropyLoss()
optimizer = optim.Adam(dnet.parameters(),lr=config['lr'])

_,_,_ = evaluate(dnet,test_loader)

history = train(dnet,train_loader,lossfunction,optimizer,n_epochs=config['EPOCHS'])
# print(dnet(torch.rand(2,3,64,64).to(device)).shape)

y_true,y_pred,pred_prob = evaluate(dnet,test_loader)

precision,recall,f_score,_ = precision_recall_fscore_support(y_true,y_pred)

print(f'precision: {np.array(precision).mean():.3f}')
print(f'recall: {np.array(recall).mean():.3f}')
print(f'f_score: {np.array(f_score).mean():.3f}')

plot_loss_acc(history)

